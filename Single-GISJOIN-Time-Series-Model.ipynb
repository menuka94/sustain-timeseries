{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from prophet import Prophet\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "jars = \"\" \\\n",
    "       \"./jars/mongo-spark-connector_2.12-3.0.1.jar,\" \\\n",
    "       \"./jars/mongo-java-driver-3.12.5.jar,\" \\\n",
    "       \"./jars/bson-4.0.5.jar,\" \\\n",
    "       \"./jars/spark-core_2.12-3.0.1.jar,\" \\\n",
    "       \"./jars/spark-sql_2.12-3.0.1.jar\"\n",
    "\n",
    "SPARK_MASTER = os.environ[\"SPARK_MASTER\"]\n",
    "DB_NAME = os.environ[\"DB_NAME\"]\n",
    "DB_HOST = os.environ[\"DB_HOST\"]\n",
    "DB_PORT = os.environ[\"DB_PORT\"]\n",
    "# SPARK_THREAD_COUNT = os.environ[\"SPARK_THREAD_COUNT\"]\n",
    "SPARK_EXECUTOR_CORES = os.environ[\"SPARK_EXECUTOR_CORES\"]\n",
    "SPARK_EXECUTOR_MEMORY = os.environ[\"SPARK_EXECUTOR_MEMORY\"]\n",
    "SPARK_INITIAL_EXECUTORS = os.environ[\"SPARK_INITIAL_EXECUTORS\"]\n",
    "SPARK_MIN_EXECUTORS = os.environ[\"SPARK_MIN_EXECUTORS\"]\n",
    "SPARK_MAX_EXECUTORS = os.environ[\"SPARK_MAX_EXECUTORS\"]\n",
    "SPARK_BACKLOG_TIMEOUT = os.environ[\"SPARK_BACKLOG_TIMEOUT\"]\n",
    "SPARK_IDLE_TIMEOUT = os.environ[\"SPARK_IDLE_TIMEOUT\"]\n",
    "\n",
    "GISJOIN = \"GISJOIN\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .appName(\"COVID-19 Time-series - PySpark\") \\\n",
    "    .config(\"spark.jars\", jars) \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .config(\"park.executor.cores\", SPARK_EXECUTOR_CORES) \\\n",
    "    .config(\"spark.executor.memory\", SPARK_EXECUTOR_MEMORY) \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", SPARK_INITIAL_EXECUTORS) \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", SPARK_MIN_EXECUTORS) \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", SPARK_MAX_EXECUTORS) \\\n",
    "    .config(\"spark.dynamicAllocation.schedulerBacklogTimeout\", SPARK_BACKLOG_TIMEOUT) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", SPARK_IDLE_TIMEOUT) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def predict(df0):\n",
    "    # instantiate the model, configure the parameters\n",
    "    print('>>> predict(): call')\n",
    "    m = Prophet()\n",
    "    m.fit(df0)\n",
    "    df0_future = m.make_future_dataframe(periods=365)\n",
    "    df0_forecast = m.predict(df0_future)\n",
    "\n",
    "    return df0_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+----------+-------------------+\n",
      "| GISJOIN|cases|deaths|      date|     formatted_date|\n",
      "+--------+-----+------+----------+-------------------+\n",
      "|G0100010|    0|     0|2020-03-27|2020-03-26 18:00:00|\n",
      "|G0100010|    3|     0|2020-03-25|2020-03-24 18:00:00|\n",
      "|G0100010|    0|     0|2020-03-28|2020-03-27 18:00:00|\n",
      "|G0100010|    0|     0|2020-03-29|2020-03-28 18:00:00|\n",
      "|G0100010|    1|     0|2020-03-30|2020-03-29 18:00:00|\n",
      "|G0100010|    0|     0|2020-03-31|2020-03-30 18:00:00|\n",
      "|G0100010|    3|     0|2020-04-01|2020-03-31 18:00:00|\n",
      "|G0100010|    2|     0|2020-03-26|2020-03-25 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-04|2020-04-03 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-05|2020-04-04 18:00:00|\n",
      "|G0100010|    0|     1|2020-04-06|2020-04-05 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-02|2020-04-01 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-07|2020-04-06 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-08|2020-04-07 18:00:00|\n",
      "|G0100010|    5|     0|2020-04-09|2020-04-08 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-10|2020-04-09 18:00:00|\n",
      "|G0100010|    2|     0|2020-04-11|2020-04-10 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-12|2020-04-11 18:00:00|\n",
      "|G0100010|    0|     0|2020-04-13|2020-04-12 18:00:00|\n",
      "|G0100010|    4|     0|2020-04-14|2020-04-13 18:00:00|\n",
      "+--------+-----+------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------+---+\n",
      "| GISJOIN|        ds|  y|\n",
      "+--------+----------+---+\n",
      "|G0100010|2020-03-27|  0|\n",
      "|G0100010|2020-03-25|  3|\n",
      "|G0100010|2020-03-28|  0|\n",
      "|G0100010|2020-03-29|  0|\n",
      "|G0100010|2020-03-30|  1|\n",
      "|G0100010|2020-03-31|  0|\n",
      "|G0100010|2020-04-01|  3|\n",
      "|G0100010|2020-03-26|  2|\n",
      "|G0100010|2020-04-04|  0|\n",
      "|G0100010|2020-04-05|  0|\n",
      "|G0100010|2020-04-06|  0|\n",
      "|G0100010|2020-04-02|  0|\n",
      "|G0100010|2020-04-07|  0|\n",
      "|G0100010|2020-04-08|  0|\n",
      "|G0100010|2020-04-09|  5|\n",
      "|G0100010|2020-04-10|  0|\n",
      "|G0100010|2020-04-11|  2|\n",
      "|G0100010|2020-04-12|  0|\n",
      "|G0100010|2020-04-13|  0|\n",
      "|G0100010|2020-04-14|  4|\n",
      "+--------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "temp = sc._jsc.sc()\n",
    "\n",
    "mongo_connection_uri = f'mongodb://{DB_HOST}:{DB_PORT}/{DB_NAME}.covid_county_formatted'\n",
    "\n",
    "df = spark.read.format(\"mongo\").option(\"uri\", mongo_connection_uri).load()\n",
    "\n",
    "df = df.select(GISJOIN, 'cases', 'deaths', 'date', 'formatted_date')\n",
    "\n",
    "df.show()\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"ds\", DateType(), True),\n",
    "    StructField(\"yhat\", DoubleType(), True),\n",
    "    StructField(\"yhat_lower\", DoubleType(), True),\n",
    "    StructField(\"yhat_upper\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df_cases = df.select(GISJOIN, 'date', 'cases').withColumnRenamed('date', 'ds').withColumnRenamed('cases', 'y')\n",
    "\n",
    "df_cases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('GISJOIN')\n",
    "p_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GISJOIN</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>date</th>\n",
       "      <th>formatted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>2020-03-26 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G0100010</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-03-24 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>2020-03-27 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>2020-03-28 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G0100010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-29 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064945</th>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>2021-02-23 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064946</th>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-04</td>\n",
       "      <td>2021-03-03 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064947</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021-03-06 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064948</th>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-04 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064949</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>2021-03-05 17:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1064950 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GISJOIN  cases  deaths        date      formatted_date\n",
       "0        G0100010      0       0  2020-03-27 2020-03-26 18:00:00\n",
       "1        G0100010      3       0  2020-03-25 2020-03-24 18:00:00\n",
       "2        G0100010      0       0  2020-03-28 2020-03-27 18:00:00\n",
       "3        G0100010      0       0  2020-03-29 2020-03-28 18:00:00\n",
       "4        G0100010      1       0  2020-03-30 2020-03-29 18:00:00\n",
       "...           ...    ...     ...         ...                 ...\n",
       "1064945      None     10       0  2021-02-24 2021-02-23 17:00:00\n",
       "1064946      None      8       0  2021-03-04 2021-03-03 17:00:00\n",
       "1064947      None      0       0  2021-03-07 2021-03-06 17:00:00\n",
       "1064948      None      4       0  2021-03-05 2021-03-04 17:00:00\n",
       "1064949      None      0       0  2021-03-06 2021-03-05 17:00:00\n",
       "\n",
       "[1064950 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_joins = df.select('GISJOIN').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_join = gis_joins[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[GISJOIN: string, cases: int, deaths: int, date: string, formatted_date: timestamp]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = df.where(df.GISJOIN == gis_join)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[GISJOIN: string, cases: int, deaths: int, date: string, formatted_date: timestamp] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7fc198998a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[GISJOIN: string, cases: int, deaths: int, date: string, formatted_date: timestamp] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "predict(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/pandas/group_ops.py:73: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_cases = df.select('GISJOIN', 'date', 'cases').withColumnRenamed('date', 'ds').withColumnRenamed('cases', 'y')\n",
    "\n",
    "results = (df_cases.groupBy('GISJOIN').apply(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
