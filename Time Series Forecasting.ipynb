{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9d028da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cumulative_cases</th>\n",
       "      <th>cumulative_deaths</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>GISJOIN</th>\n",
       "      <th>formatted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605420df8dd202bf891eb049</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>2020-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605420df8dd202bf891eb04a</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>2020-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>605420df8dd202bf891eb04b</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>2020-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>605420df8dd202bf891eb04c</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01001</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>2020-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>605420df8dd202bf891eb04d</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>01001</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064945</th>\n",
       "      <td>605420f18dd202bf892ee89b</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043</td>\n",
       "      <td>107</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>2020-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064946</th>\n",
       "      <td>605420f18dd202bf892ee89c</td>\n",
       "      <td>2020-09-04</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043</td>\n",
       "      <td>109</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>2020-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064947</th>\n",
       "      <td>605420f18dd202bf892ee89d</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043</td>\n",
       "      <td>107</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>2020-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064948</th>\n",
       "      <td>605420f18dd202bf892ee89e</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043</td>\n",
       "      <td>107</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>2020-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064949</th>\n",
       "      <td>605420f18dd202bf892ee89f</td>\n",
       "      <td>2020-09-05</td>\n",
       "      <td>Washakie</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>56043</td>\n",
       "      <td>109</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>2020-09-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1064950 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              _id        date    county    state   fips  \\\n",
       "0        605420df8dd202bf891eb049  2020-03-25   Autauga  Alabama  01001   \n",
       "1        605420df8dd202bf891eb04a  2020-03-28   Autauga  Alabama  01001   \n",
       "2        605420df8dd202bf891eb04b  2020-03-29   Autauga  Alabama  01001   \n",
       "3        605420df8dd202bf891eb04c  2020-03-30   Autauga  Alabama  01001   \n",
       "4        605420df8dd202bf891eb04d  2020-03-31   Autauga  Alabama  01001   \n",
       "...                           ...         ...       ...      ...    ...   \n",
       "1064945  605420f18dd202bf892ee89b  2020-08-30  Washakie  Wyoming  56043   \n",
       "1064946  605420f18dd202bf892ee89c  2020-09-04  Washakie  Wyoming  56043   \n",
       "1064947  605420f18dd202bf892ee89d  2020-08-31  Washakie  Wyoming  56043   \n",
       "1064948  605420f18dd202bf892ee89e  2020-08-29  Washakie  Wyoming  56043   \n",
       "1064949  605420f18dd202bf892ee89f  2020-09-05  Washakie  Wyoming  56043   \n",
       "\n",
       "         cumulative_cases  cumulative_deaths  cases  deaths   GISJOIN  \\\n",
       "0                       4                0.0      3       0  G0100010   \n",
       "1                       6                0.0      0       0  G0100010   \n",
       "2                       6                0.0      0       0  G0100010   \n",
       "3                       7                0.0      1       0  G0100010   \n",
       "4                       7                0.0      0       0  G0100010   \n",
       "...                   ...                ...    ...     ...       ...   \n",
       "1064945               107                5.0      0       0  G5600430   \n",
       "1064946               109                6.0      1       0  G5600430   \n",
       "1064947               107                5.0      0       0  G5600430   \n",
       "1064948               107                5.0      0       0  G5600430   \n",
       "1064949               109                6.0      0       0  G5600430   \n",
       "\n",
       "        formatted_date  \n",
       "0           2020-03-25  \n",
       "1           2020-03-28  \n",
       "2           2020-03-29  \n",
       "3           2020-03-30  \n",
       "4           2020-03-31  \n",
       "...                ...  \n",
       "1064945     2020-08-30  \n",
       "1064946     2020-09-04  \n",
       "1064947     2020-08-31  \n",
       "1064948     2020-08-29  \n",
       "1064949     2020-09-05  \n",
       "\n",
       "[1064950 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from pymongo import MongoClient\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "db = MongoClient('localhost', 27017)\n",
    "collection = 'covid_county_formatted'\n",
    "cursor = db.sustaindb[collection].find()\n",
    "df = pd.DataFrame(list(cursor))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02518735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GISJOINs: 3116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>GISJOIN</th>\n",
       "      <th>y</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>2020-09-12</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ds   GISJOIN   y  deaths\n",
       "0     2020-03-25  G0100010   3       0\n",
       "1     2020-03-28  G0100010   0       0\n",
       "2     2020-03-29  G0100010   0       0\n",
       "3     2020-03-30  G0100010   1       0\n",
       "4     2020-03-31  G0100010   0       0\n",
       "...          ...       ...  ..     ...\n",
       "3150  2020-09-07  G0100010   6       0\n",
       "3151  2020-09-10  G0100010  22       0\n",
       "3152  2020-09-11  G0100010   7       0\n",
       "3153  2020-09-12  G0100010  14       0\n",
       "3154  2020-09-14  G0100010   9       0\n",
       "\n",
       "[349 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"date\", \"GISJOIN\", \"cases\", \"deaths\"]]\n",
    "\n",
    "all_gis_joins = df['GISJOIN'].unique()\n",
    "print(f'#GISJOINs: {len(all_gis_joins)}')\n",
    "\n",
    "df0 = df.loc[df['GISJOIN'] == all_gis_joins[0]]\n",
    "df0 = df0.rename(columns = {\n",
    "    'date': 'ds',\n",
    "    'cases': 'y'\n",
    "})\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35ad02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>yhat</th>\n",
       "      <th>yhat_lower</th>\n",
       "      <th>yhat_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>5.928523</td>\n",
       "      <td>-13.826868</td>\n",
       "      <td>24.950116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.538198</td>\n",
       "      <td>-16.387674</td>\n",
       "      <td>22.758285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>4.551130</td>\n",
       "      <td>-15.462555</td>\n",
       "      <td>23.495693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>2.084051</td>\n",
       "      <td>-19.189924</td>\n",
       "      <td>21.845346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>0.476235</td>\n",
       "      <td>-19.121344</td>\n",
       "      <td>19.919808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>-45.358355</td>\n",
       "      <td>-102.597465</td>\n",
       "      <td>9.077196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>-48.100980</td>\n",
       "      <td>-104.250275</td>\n",
       "      <td>4.043099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>2022-03-05</td>\n",
       "      <td>-49.984343</td>\n",
       "      <td>-102.835298</td>\n",
       "      <td>3.868638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>2022-03-06</td>\n",
       "      <td>-49.708821</td>\n",
       "      <td>-102.459656</td>\n",
       "      <td>4.882562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>-53.666535</td>\n",
       "      <td>-109.203797</td>\n",
       "      <td>-0.424355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds       yhat  yhat_lower  yhat_upper\n",
       "0   2020-03-24   5.928523  -13.826868   24.950116\n",
       "1   2020-03-25   2.538198  -16.387674   22.758285\n",
       "2   2020-03-26   4.551130  -15.462555   23.495693\n",
       "3   2020-03-27   2.084051  -19.189924   21.845346\n",
       "4   2020-03-28   0.476235  -19.121344   19.919808\n",
       "..         ...        ...         ...         ...\n",
       "709 2022-03-03 -45.358355 -102.597465    9.077196\n",
       "710 2022-03-04 -48.100980 -104.250275    4.043099\n",
       "711 2022-03-05 -49.984343 -102.835298    3.868638\n",
       "712 2022-03-06 -49.708821 -102.459656    4.882562\n",
       "713 2022-03-07 -53.666535 -109.203797   -0.424355\n",
       "\n",
       "[714 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Prophet(\n",
    "    interval_width=0.95,\n",
    "    growth='linear',\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    ")\n",
    "\n",
    "# model.fit(df0)\n",
    "m = Prophet()\n",
    "m.fit(df0)\n",
    "df0_future = m.make_future_dataframe(periods=365)\n",
    "df0_forecast = m.predict(df0_future)\n",
    "df0_forecast\n",
    "\n",
    "df0_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04dc06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.plot_components(df0_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bfd0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objs as go\n",
    "# plot_plotly(m, df0_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94b55b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"ds\", DateType(), True),\n",
    "    StructField(\"yhat\", DoubleType(), True),\n",
    "    StructField(\"yhat_lower\", DoubleType(), True),\n",
    "    StructField(\"yhat_upper\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def forecast_store_item(history_pd):\n",
    "\n",
    "    # instantiate the model, configure the parameters\n",
    "    model = Prophet()\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(history_pd)\n",
    "\n",
    "    # configure predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=90,\n",
    "        freq='d',\n",
    "        include_history=True\n",
    "    )\n",
    "\n",
    "    # make predictions\n",
    "    results_pd = model.predict(future_pd)\n",
    "\n",
    "    # . . .\n",
    "\n",
    "    # return predictions\n",
    "    return results_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f548eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = StructType([\n",
    "    StructField(\"ds\", DateType(), True),\n",
    "    StructField(\"yhat\", DoubleType(), True),\n",
    "    StructField(\"yhat_lower\", DoubleType(), True),\n",
    "    StructField(\"yhat_upper\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def temp(history_pd):\n",
    "\n",
    "    # instantiate the model, configure the parameters\n",
    "    m = Prophet()\n",
    "    m.fit(df0)\n",
    "    df0_future = m.make_future_dataframe(periods=365)\n",
    "    df0_forecast = m.predict(df0_future)\n",
    "\n",
    "    return df0_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51816f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = df0[[\"ds\", \"y\"]]\n",
    "# df0.set_index('ds')\n",
    "# temp(df0[[\"ds\", \"GISJOIN\", \"y\", \"deaths\"]])\n",
    "# df0\n",
    "\n",
    "# results = (\n",
    "#     df.groupBy('GISJOIN')\n",
    "#     .apply(temp)\n",
    "# )\n",
    "# df0_ = df0.withColumn('GISJOIN', lit(None))\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder\\\n",
    "#     .master('local')\\\n",
    "#     .config('spark.mongodb.input.uri', 'mongodb://localhost:27017/sustaindb.covid_county_formatted')\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# spark_df = spark.read.format(\"mongo\").option(\"uri\",\n",
    "#     \"mongodb://localhost/sustaindb.covid_county_formatted\").load()\n",
    "# spark_df\n",
    "#     .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1.jar')\\\n",
    "\n",
    "# spark.sparkContext.addPyFile('./jars/mongo-spark-connector_2.12-3.0.1.jar')\n",
    "# spark.sparkContext.addPyFile('./jars/spark-core_2.12-3.0.1.jar')\n",
    "# spark.sparkContext.addPyFile('./jars/spark-sql_2.12-3.0.1.jar')\n",
    "# spark.sparkContext.addPyFile('./jars/spark-mllib_2.12-3.0.1.jar')\n",
    "# spark.sparkContext.addPyFile('./jars/bson-4.0.5.jar')\n",
    "# spark.sparkContext.addPyFile('./jars/mongo-java-driver-3.12.5.jar')\n",
    "\n",
    "# df1 = spark.read.format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "#             .option('database', 'sustaindb')\\\n",
    "#             .option('collection', 'covid_county_formatted')\\\n",
    "#             .load()\n",
    "\n",
    "df = spark.read.format('json').load('./covid_county.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecc29bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+----------+--------------------+\n",
      "| GISJOIN|cases|deaths|      date|      formatted_date|\n",
      "+--------+-----+------+----------+--------------------+\n",
      "|G0100010|    3|     0|2020-03-25|[2020-03-25T00:00...|\n",
      "|G0100010|    0|     0|2020-03-28|[2020-03-28T00:00...|\n",
      "|G0100010|    0|     0|2020-03-29|[2020-03-29T00:00...|\n",
      "|G0100010|    1|     0|2020-03-30|[2020-03-30T00:00...|\n",
      "|G0100010|    0|     0|2020-03-31|[2020-03-31T00:00...|\n",
      "|G0100010|    3|     0|2020-04-01|[2020-04-01T00:00...|\n",
      "|G0100010|    0|     0|2020-04-04|[2020-04-04T00:00...|\n",
      "|G0100010|    0|     0|2020-04-05|[2020-04-05T00:00...|\n",
      "|G0100010|    0|     1|2020-04-06|[2020-04-06T00:00...|\n",
      "|G0100010|    0|     0|2020-04-02|[2020-04-02T00:00...|\n",
      "|G0100010|    0|     0|2020-04-07|[2020-04-07T00:00...|\n",
      "|G0100010|    0|     0|2020-04-08|[2020-04-08T00:00...|\n",
      "|G0100010|    5|     0|2020-04-09|[2020-04-09T00:00...|\n",
      "|G0100010|    0|     0|2020-04-10|[2020-04-10T00:00...|\n",
      "|G0100010|    2|     0|2020-04-11|[2020-04-11T00:00...|\n",
      "|G0100010|    0|     0|2020-04-12|[2020-04-12T00:00...|\n",
      "|G0100010|    0|     0|2020-04-13|[2020-04-13T00:00...|\n",
      "|G0100010|   16|     0|2020-09-15|[2020-09-15T00:00...|\n",
      "|G0100010|   18|     0|2020-09-16|[2020-09-16T00:00...|\n",
      "|G0100010|    5|     0|2020-09-17|[2020-09-17T00:00...|\n",
      "+--------+-----+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('GISJOIN', 'cases', 'deaths', 'date', 'formatted_date')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d20e0354",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o530.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 44, 10.253.202.195, executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4df78ca389ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/6FC71512346BCBD9/github/sustain-timeseries/venv39/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o530.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 44, 10.253.202.195, executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:490)\n\tat io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)\n\tat io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)\n\tat io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)\n\tat org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:240)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:120)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:94)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:101)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
     ]
    }
   ],
   "source": [
    "# df0 = df0.rename(columns = {\n",
    "#     'date': 'ds',\n",
    "#     'cases': 'y'\n",
    "# })\n",
    "df_cases = df.select('GISJOIN', 'date', 'cases').withColumnRenamed('date', 'ds').withColumnRenamed('cases', 'y')\n",
    "# df_cases.show()\n",
    "\n",
    "results = (\n",
    "    df_cases.groupBy('GISJOIN')\n",
    "    .apply(temp)\n",
    ")\n",
    "results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d568f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql import SparkSession \n",
    "\n",
    " spark = SparkSession.builder.master(\"local\").getOrCreate() \n",
    " print(spark.sparkContext.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
